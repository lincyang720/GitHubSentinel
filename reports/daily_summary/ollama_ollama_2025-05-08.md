# Project Update for `ollama/ollama` - 2025-05-08

## Summary
Today, we have 8 open issues and 10 open pull requests in the project. The issues range from the V100s not being used to problems with the f-lite image generation model. On the pull requests side, we have updates ranging from documentation improvements to performance optimization and bug fixes.

## Key Issues
- Issue #10617: V100s not Being Used (reported by sempervictus)
- Issue #10616: Create Development Network Testing Model (reported by randomchristiancoder)
- Issue #10615: mistral-small3.1:24b q4 use 100% CPU when change num_ctx to 128K (reported by Seraphli)
- Issue #10613: Option to Mask or Remove Logged Queries for Privacy and Log Size Optimization (reported by ALLMI78)
- Issue #10612: phi4-reasoning:14b-q4_K_M extremely slow compared to other 14B models (reported by ALLMI78)

## Important PRs
- PR #10610: Recommend manual installs go to `/usr/local` in docs (proposed by strugee)
- PR #10609: Add BlazeLama to community section (proposed by HardCodeDev777)
- PR #10596: CPU Model Performance Optimization for Ollama (proposed by WingsDrafterwork)
- PR #10594: Enable usetesting, disable tenv in lint (proposed by mxyng)
- PR #10593: Fix stream accumulator exits early (proposed by mxyng)