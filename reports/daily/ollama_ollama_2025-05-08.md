# üìå Daily Update for `ollama/ollama` - 2025-05-08

## üî® Recent Commits
- [3d9498a] ollamarunner: Use correct constant to remove cache entries (by @jessegross)
- [3098c8b] CI: trigger downstream release process (#10508) (by @dhiltgen)
- [5e380c3] sched: fix race leading to orphaned runners (#10599) (by @dhiltgen)
- [392de84] api: remove unused RetrieveModelResponse type (#10603) (by @jmorganca)
- [af31cce] fix data race in WriteGGUF (#10598) (by @dhiltgen)

## üêû Open Issues
- #10617 V100s not Being Used (sempervictus)
- #10616 Create Development Network Testing Model (randomchristiancoder)
- #10615 mistral-small3.1:24b q4 use 100% CPU when change num_ctx to 128K (Seraphli)
- #10613 Option to Mask or Remove Logged Queries for Privacy and Log Size Optimization (ALLMI78)
- #10612 phi4-reasoning:14b-q4_K_M extremely slow compared to other 14B models (ALLMI78)
- #10611 Safetensors import of Gemma3 fails quantization > 0.6.5 (NinjaLane)
- #10608 Version 0.68: When using langchain4j to call the model deployed by ollama and the function tool, the streaming output does not take effect (zhangsic-wlf)
- #10606 f-lite image generation model (erickeller)

## üîÅ Open Pull Requests
- #10610 docs: recommend manual installs go to `/usr/local` (strugee)
- #10609 Add BlazeLama to community section (HardCodeDev777)
- #10596 CPU Model Performance Optimization for Ollama (WingsDrafterwork)
- #10594 lint: enable usetesting, disable tenv (mxyng)
- #10593 fix: stream accumulator exits early (mxyng)
- #10589 docs: add OllamaChat to community integrations (rijieli)
- #10584 WIP thinking API support (drifkin)
- #10581 api: remove unused sampling parameters (jmorganca)
- #10577 model: handle multiple eos tokens (mxyng)
- #10537 docs: added OpenLLMetry link (nirga)
