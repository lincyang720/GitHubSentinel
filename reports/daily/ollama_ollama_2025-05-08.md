# üìå Daily Update for `ollama/ollama` - 2025-05-08

## üêû Open Issues
- #10617 V100s not Being Used (sempervictus)
- #10616 Create Development Network Testing Model (randomchristiancoder)
- #10615 mistral-small3.1:24b q4 use 100% CPU when change num_ctx to 128K (Seraphli)
- #10613 Option to Mask or Remove Logged Queries for Privacy and Log Size Optimization (ALLMI78)
- #10612 phi4-reasoning:14b-q4_K_M extremely slow compared to other 14B models (ALLMI78)
- #10611 Safetensors import of Gemma3 fails quantization > 0.6.5 (NinjaLane)
- #10608 Version 0.68: When using langchain4j to call the model deployed by ollama and the function tool, the streaming output does not take effect (zhangsic-wlf)
- #10606 f-lite image generation model (erickeller)

## üîÅ Open Pull Requests
- #10610 docs: recommend manual installs go to `/usr/local` (strugee)
- #10609 Add BlazeLama to community section (HardCodeDev777)
- #10596 CPU Model Performance Optimization for Ollama (WingsDrafterwork)
- #10594 lint: enable usetesting, disable tenv (mxyng)
- #10593 fix: stream accumulator exits early (mxyng)
- #10589 docs: add OllamaChat to community integrations (rijieli)
- #10584 WIP thinking API support (drifkin)
- #10581 api: remove unused sampling parameters (jmorganca)
- #10577 model: handle multiple eos tokens (mxyng)
- #10537 docs: added OpenLLMetry link (nirga)
